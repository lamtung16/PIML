{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jzH_LbEIGLWu"
      },
      "outputs": [],
      "source": [
        "# library\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torchmetrics import R2Score\n",
        "\n",
        "r2score = R2Score()\n",
        "\n",
        "torch.manual_seed(1)\n",
        "np.random.seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Y3eaKBL2GhJI"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "class Net(torch.nn.Module):\n",
        "  def __init__(self, h1, h2):\n",
        "    super(Net, self).__init__()\n",
        "    self.input   = torch.nn.Linear(3, h1)\n",
        "    self.hidden1 = torch.nn.Linear(h1, h2)\n",
        "    self.output  = torch.nn.Linear(h2, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    z = torch.selu(self.input(x))\n",
        "    z = torch.selu(self.hidden1(z))\n",
        "    z = torch.relu(self.output(z))\n",
        "    return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BtNtb3epPIzJ"
      },
      "outputs": [],
      "source": [
        "# Model evaluation\n",
        "def eval(model, testset):\n",
        "    with torch.no_grad():\n",
        "        pred_Y = model(testset.x_data)\n",
        "    \n",
        "    r2 = r2score(pred_Y, testset.y_data)\n",
        "    return r2.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dTWtNEdVHCuw"
      },
      "outputs": [],
      "source": [
        "# Data class\n",
        "class Data(torch.utils.data.Dataset):\n",
        "  def __init__(self, src_file, start=None, end=None):\n",
        "    df = pd.read_csv(src_file)\n",
        "    Tz_k   = np.array(df['room_temp']).reshape(-1,1)[start: end]\n",
        "    Tsa_k1 = np.array(df['supply_discharge_temp']).reshape(-1,1)[start+1: end+1]\n",
        "    msa_k1 = np.array(df['airflow_current']).reshape(-1,1)[start+1: end+1]\n",
        "\n",
        "    tmp_x = np.concatenate((Tz_k, Tsa_k1, msa_k1), axis=1)\n",
        "    tmp_y = np.array(df['room_temp']).reshape(-1,1)[start+1: end+1]\n",
        "\n",
        "    self.x_data = torch.tensor(tmp_x, dtype=torch.float32)\n",
        "    self.y_data = torch.tensor(tmp_y, dtype=torch.float32)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.x_data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if torch.is_tensor(idx):\n",
        "      idx = idx.tolist()\n",
        "    inp  = self.x_data[idx]\n",
        "    outp = self.y_data[idx]\n",
        "    sample = {'inp':inp, 'outp':outp}\n",
        "    return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Early stopping function\n",
        "def early_stop(list, min_epochs, patience):\n",
        "    if(len(list) > min_epochs):\n",
        "        if(np.max(list[-patience:]) < 1.00001*np.max(list[0: -patience])):\n",
        "            return 1\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gebr4CnRLFBd",
        "outputId": "d74e7cd7-8dab-49a9-babc-e77577dd73b7"
      },
      "outputs": [],
      "source": [
        "# train function\n",
        "def train(net, train_ds, test_ds, lr=0.001, min_epochs=200, max_epochs=100000, patience=100, smooth=0):\n",
        "    loss_func  = torch.nn.MSELoss()\n",
        "    optimizer  = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "    R2_test = np.array([])\n",
        "    train_ldr = torch.utils.data.DataLoader(train_ds, batch_size=train_ds.y_data.shape[0], shuffle=True)\n",
        "    for _ in range(0, max_epochs+1):\n",
        "        net.train()\n",
        "        for (_, batch) in enumerate(train_ldr):\n",
        "            X = batch['inp']\n",
        "            Y = batch['outp']\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = net(X)\n",
        "            loss_val = loss_func(output, Y) + smooth*loss_func(output, X[:,0].reshape(-1,1))\n",
        "            loss_val.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        net.eval()\n",
        "        R2_test = np.append(R2_test, eval(net, test_ds))\n",
        "        \n",
        "        if(early_stop(list = R2_test, min_epochs = min_epochs, patience = patience) == 1):\n",
        "            break\n",
        "    \n",
        "    return R2_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     n_train  smooth  size      lr  best_epoch         R2\n",
            "0       16.0   0.000   3.0  0.0001         3.0 -1143.4139\n",
            "1       16.0   0.000   3.0  0.0010         0.0 -1524.0552\n",
            "2       16.0   0.000   3.0  0.0100         0.0  -188.3574\n",
            "3       16.0   0.000   4.0  0.0001         0.0 -1524.0552\n",
            "4       16.0   0.000   4.0  0.0010         0.0  -204.8016\n",
            "5       16.0   0.000   4.0  0.0100         0.0 -1524.0552\n",
            "6       16.0   0.000  16.0  0.0001         0.0 -1524.0552\n",
            "7       16.0   0.000  16.0  0.0010         3.0  -187.3654\n",
            "8       16.0   0.000  16.0  0.0100         0.0  -145.4236\n",
            "9       16.0   0.001   3.0  0.0001         3.0 -1114.7590\n",
            "10      16.0   0.001   3.0  0.0010         0.0 -1523.7240\n",
            "11      16.0   0.001   3.0  0.0100         3.0 -1457.0485\n",
            "12      16.0   0.001   4.0  0.0001         3.0  -530.0001\n",
            "13      16.0   0.001   4.0  0.0010         3.0 -1199.2279\n",
            "14      16.0   0.001   4.0  0.0100         0.0 -1524.0552\n",
            "15      16.0   0.001  16.0  0.0001         0.0 -1524.0552\n",
            "16      16.0   0.001  16.0  0.0010         3.0  -646.5138\n",
            "17      16.0   0.001  16.0  0.0100         0.0 -1524.0552\n",
            "18      16.0   0.010   3.0  0.0001         0.0 -1524.0552\n",
            "19      16.0   0.010   3.0  0.0010         0.0 -1524.0552\n",
            "20      16.0   0.010   3.0  0.0100         0.0 -1524.0552\n",
            "21      16.0   0.010   4.0  0.0001         0.0 -1524.0552\n",
            "22      16.0   0.010   4.0  0.0010         0.0 -1524.0552\n",
            "23      16.0   0.010   4.0  0.0100         0.0 -1524.0552\n",
            "24      16.0   0.010  16.0  0.0001         3.0  -808.6304\n",
            "25      16.0   0.010  16.0  0.0010         3.0  -721.6630\n",
            "26      16.0   0.010  16.0  0.0100         0.0 -1524.0552\n",
            "27      32.0   0.000   3.0  0.0001         3.0 -1502.1842\n",
            "28      32.0   0.000   3.0  0.0010         0.0 -1533.6947\n",
            "29      32.0   0.000   3.0  0.0100         0.0 -1533.6947\n",
            "30      32.0   0.000   4.0  0.0001         3.0 -1459.7720\n",
            "31      32.0   0.000   4.0  0.0010         0.0 -1533.6947\n",
            "32      32.0   0.000   4.0  0.0100         0.0 -1533.6947\n",
            "33      32.0   0.000  16.0  0.0001         3.0  -713.8146\n",
            "34      32.0   0.000  16.0  0.0010         0.0 -1532.4912\n",
            "35      32.0   0.000  16.0  0.0100         0.0 -1533.6947\n",
            "36      32.0   0.001   3.0  0.0001         0.0 -1533.6947\n",
            "37      32.0   0.001   3.0  0.0010         3.0  -775.0349\n",
            "38      32.0   0.001   3.0  0.0100         0.0 -1533.6947\n",
            "39      32.0   0.001   4.0  0.0001         3.0 -1261.0703\n",
            "40      32.0   0.001   4.0  0.0010         0.0 -1533.6947\n",
            "41      32.0   0.001   4.0  0.0100         0.0 -1533.6947\n",
            "42      32.0   0.001  16.0  0.0001         0.0  -157.9043\n",
            "43      32.0   0.001  16.0  0.0010         3.0  -764.0585\n",
            "44      32.0   0.001  16.0  0.0100         2.0  -135.6740\n",
            "45      32.0   0.010   3.0  0.0001         0.0 -1533.6947\n",
            "46      32.0   0.010   3.0  0.0010         3.0 -1465.6443\n",
            "47      32.0   0.010   3.0  0.0100         3.0 -1488.7690\n",
            "48      32.0   0.010   4.0  0.0001         0.0 -1533.6947\n",
            "49      32.0   0.010   4.0  0.0010         0.0 -1533.6947\n",
            "50      32.0   0.010   4.0  0.0100         1.0  -178.9170\n",
            "51      32.0   0.010  16.0  0.0001         0.0 -1533.6947\n",
            "52      32.0   0.010  16.0  0.0010         3.0  -898.5756\n",
            "53      32.0   0.010  16.0  0.0100         0.0 -1533.6947\n",
            "54      64.0   0.000   3.0  0.0001         0.0 -1507.4968\n",
            "55      64.0   0.000   3.0  0.0010         3.0  -847.5104\n",
            "56      64.0   0.000   3.0  0.0100         3.0 -1475.9769\n",
            "57      64.0   0.000   4.0  0.0001         0.0 -1507.4968\n",
            "58      64.0   0.000   4.0  0.0010         0.0 -1507.4968\n",
            "59      64.0   0.000   4.0  0.0100         3.0  -127.6987\n",
            "60      64.0   0.000  16.0  0.0001         3.0  -186.1752\n",
            "61      64.0   0.000  16.0  0.0010         3.0 -1057.0100\n",
            "62      64.0   0.000  16.0  0.0100         0.0 -1507.4968\n",
            "63      64.0   0.001   3.0  0.0001         0.0 -1507.4968\n",
            "64      64.0   0.001   3.0  0.0010         0.0 -1507.4968\n",
            "65      64.0   0.001   3.0  0.0100         3.0  -533.3028\n",
            "66      64.0   0.001   4.0  0.0001         0.0 -1507.4968\n",
            "67      64.0   0.001   4.0  0.0010         3.0 -1025.3215\n",
            "68      64.0   0.001   4.0  0.0100         0.0  -109.9850\n",
            "69      64.0   0.001  16.0  0.0001         3.0 -1468.8097\n",
            "70      64.0   0.001  16.0  0.0010         0.0 -1507.4968\n",
            "71      64.0   0.001  16.0  0.0100         1.0  -134.5214\n",
            "72      64.0   0.010   3.0  0.0001         3.0 -1083.0360\n",
            "73      64.0   0.010   3.0  0.0010         0.0 -1507.4968\n",
            "74      64.0   0.010   3.0  0.0100         1.0   -75.4691\n",
            "75      64.0   0.010   4.0  0.0001         3.0  -456.9289\n",
            "76      64.0   0.010   4.0  0.0010         0.0 -1507.4968\n",
            "77      64.0   0.010   4.0  0.0100         1.0  -136.3264\n",
            "78      64.0   0.010  16.0  0.0001         0.0 -1507.4968\n",
            "79      64.0   0.010  16.0  0.0010         3.0  -538.9761\n",
            "80      64.0   0.010  16.0  0.0100         0.0 -1507.4968\n",
            "81     128.0   0.000   3.0  0.0001         0.0 -1514.8362\n",
            "82     128.0   0.000   3.0  0.0010         0.0 -1514.8362\n",
            "83     128.0   0.000   3.0  0.0100         0.0 -1514.8362\n",
            "84     128.0   0.000   4.0  0.0001         3.0 -1029.7535\n",
            "85     128.0   0.000   4.0  0.0010         3.0 -1222.7941\n",
            "86     128.0   0.000   4.0  0.0100         3.0  -113.6054\n",
            "87     128.0   0.000  16.0  0.0001         3.0  -662.0043\n",
            "88     128.0   0.000  16.0  0.0010         3.0  -144.8125\n",
            "89     128.0   0.000  16.0  0.0100         1.0  -134.2708\n",
            "90     128.0   0.001   3.0  0.0001         3.0 -1288.2842\n",
            "91     128.0   0.001   3.0  0.0010         0.0 -1514.8362\n",
            "92     128.0   0.001   3.0  0.0100         0.0 -1514.8362\n",
            "93     128.0   0.001   4.0  0.0001         0.0 -1514.8362\n",
            "94     128.0   0.001   4.0  0.0010         3.0 -1022.6094\n",
            "95     128.0   0.001   4.0  0.0100         3.0  -182.7484\n",
            "96     128.0   0.001  16.0  0.0001         3.0  -723.7144\n",
            "97     128.0   0.001  16.0  0.0010         3.0  -435.7754\n",
            "98     128.0   0.001  16.0  0.0100         0.0 -1514.8362\n",
            "99     128.0   0.010   3.0  0.0001         0.0 -1514.8362\n",
            "100    128.0   0.010   3.0  0.0010         0.0 -1514.8362\n",
            "101    128.0   0.010   3.0  0.0100         0.0 -1514.8362\n",
            "102    128.0   0.010   4.0  0.0001         3.0 -1492.0646\n",
            "103    128.0   0.010   4.0  0.0010         3.0 -1017.2098\n",
            "104    128.0   0.010   4.0  0.0100         0.0 -1514.8362\n",
            "105    128.0   0.010  16.0  0.0001         3.0  -187.5775\n",
            "106    128.0   0.010  16.0  0.0010         3.0  -554.6200\n",
            "107    128.0   0.010  16.0  0.0100         0.0 -1514.8362\n"
          ]
        }
      ],
      "source": [
        "# main\n",
        "df_result = pd.DataFrame({'n_train':[], 'smooth':[], 'size':[], 'lr':[], 'best_epoch':[], 'R2':[]})\n",
        "for _n_train in [16, 32, 64, 128]:\n",
        "    for _smooth in [0, 0.001, 0.01]:\n",
        "        for h in [3, 4, 16]:\n",
        "            for _lr in [0.0001, 0.001, 0.01]:\n",
        "\n",
        "                # Create network\n",
        "                device = torch.device(\"cpu\")\n",
        "                net = Net(h, h).to(device)\n",
        "\n",
        "                # Create Dataset and DataLoader objects\n",
        "                src_file = 'C:/Users/tln229/Downloads/Python/1. Building/data/HVAC_B90_102_exp_10m_20210424.csv'\n",
        "                n_train  = _n_train\n",
        "                train_ds = Data(src_file, start=0,       end=n_train)\n",
        "                test_ds  = Data(src_file, start=n_train, end=1600)\n",
        "\n",
        "                # train\n",
        "                R2_test = train(net, train_ds, test_ds, lr=_lr, min_epochs=300, max_epochs=50000, patience=200, smooth=_smooth)\n",
        "\n",
        "                # results\n",
        "                # print('n train = %3d \\t smooth = %6.4f \\t layer size = %2d \\t lr = %6.4f \\t best_epoch = %5d \\t best_R2 = %7.5f'\n",
        "                #     % (_n_train, _smooth, h, _lr, np.argmax(R2_test), np.max(R2_test)))\n",
        "                df_result.loc[len(df_result)] = [_n_train, _smooth, h, _lr, np.argmax(R2_test), np.max(R2_test)]\n",
        "\n",
        "with pd.option_context('display.max_rows', None,\n",
        "                       'display.max_columns', None,\n",
        "                       'display.precision', 4,\n",
        "                       ):\n",
        "    print(df_result)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "3. T_z .ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5 (tags/v3.10.5:f377153, Jun  6 2022, 16:14:13) [MSC v.1929 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "912d6611990680b3d240e982c9d50f3da4c776707cfd42695cf7d82c88d80956"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
