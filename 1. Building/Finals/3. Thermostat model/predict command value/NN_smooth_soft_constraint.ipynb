{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4jzKgPZ5D81V"
      },
      "outputs": [],
      "source": [
        "# library\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "torch.manual_seed(1)\n",
        "np.random.seed(1)\n",
        "torch.set_printoptions(precision=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HAEV0sGeEEAE"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, s):\n",
        "        super(Net, self).__init__()\n",
        "        self.input   = torch.nn.Linear(3, s)\n",
        "        self.output  = torch.nn.Linear(s, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = torch.tanh(self.input(x))\n",
        "        z = self.output(z)\n",
        "        return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8x017P0OEFVH"
      },
      "outputs": [],
      "source": [
        "# Model error\n",
        "def eval(model, testset, p):\n",
        "    with torch.no_grad():\n",
        "        pred_Y = model(testset.x_data)\n",
        "    \n",
        "    # R^2 qrh\n",
        "    r2_qrh = r2_score(testset.y_data, pred_Y)\n",
        "\n",
        "    # R^2 msa\n",
        "    pred_msa = pred_Y*p[0] + p[1]\n",
        "    r2_msa = r2_score(testset.msa_data, pred_msa)\n",
        "    \n",
        "    return r2_qrh, r2_msa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GjhVDtdREKfV"
      },
      "outputs": [],
      "source": [
        "# Data\n",
        "class Data(torch.utils.data.Dataset):\n",
        "  def __init__(self, src_file, start=None, end=None):\n",
        "    df = pd.read_csv(src_file)\n",
        "    clg_sp       = np.array(df['clg_sp_current']).reshape(-1,1)[start: end]\n",
        "    htg_sp       = np.array(df['htg_sp_current']).reshape(-1,1)[start: end]\n",
        "    htg_clg_mode = 1*np.array(df['htg_clg_mode']).reshape(-1,1)[start: end]\n",
        "\n",
        "    sp_k   = htg_sp*htg_clg_mode + clg_sp*(1-htg_clg_mode)\n",
        "    Tz_k   = np.array(df['thermostat_room_temp']).reshape(-1,1)[start: end]\n",
        "    qrh_k  = np.array(df['htg_valve_command']).reshape(-1,1)[start: end]\n",
        "    qrh_k1 = np.array(df['htg_valve_command']).reshape(-1,1)[start+1: end+1]\n",
        "    msa_k1 = np.array(df['airflow_desired']).reshape(-1,1)[start+1: end+1]\n",
        "    tmp_x  = np.concatenate((sp_k, Tz_k, qrh_k), axis=1)\n",
        "    \n",
        "    self.x_data   = torch.tensor(tmp_x,  dtype=torch.float32)\n",
        "    self.y_data   = torch.tensor(qrh_k1, dtype=torch.float32)\n",
        "    self.msa_data = torch.tensor(msa_k1, dtype=torch.float32)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.x_data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if torch.is_tensor(idx):\n",
        "      idx = idx.tolist()\n",
        "    inp  = self.x_data[idx]\n",
        "    outp = self.y_data[idx]\n",
        "    msa  = self.msa_data[idx]\n",
        "    sample = {'inp':inp, 'outp':outp, 'msa':msa}\n",
        "    return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Early stopping\n",
        "def early_stop(list, min_epochs, patience):\n",
        "    if(len(list) > min_epochs):\n",
        "        if(np.max(list[-patience:]) < 1.0001*np.max(list[0: -patience])):\n",
        "            return 1\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNRSA74gELy0",
        "outputId": "81e6dec5-e0ee-448f-c67b-22411ed38c4e"
      },
      "outputs": [],
      "source": [
        "# train function\n",
        "def train(net, p, train_ds, test_ds, lr=0.001, min_epochs=200, max_epochs=100000, patience=100, smooth=0, soft=0):\n",
        "    \n",
        "    loss_func  = torch.nn.MSELoss()\n",
        "    optimizer  = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "    train_ldr = torch.utils.data.DataLoader(train_ds, batch_size=256, shuffle=True)\n",
        "    R2_qrh = np.array([])\n",
        "    R2_msa = np.array([])\n",
        "    for _ in range(0, max_epochs+1):\n",
        "        net.train()\n",
        "        loss  = 0\n",
        "        count = 0\n",
        "        for (_, batch) in enumerate(train_ldr):\n",
        "            X    = batch['inp']\n",
        "            Y    = batch['outp']\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = net(X)                    # compute the output of the Network\n",
        "            loss_val = loss_func(output, Y) + smooth*loss_func(output, X[:,2].reshape(-1,1)) + soft*torch.sum(torch.square(torch.relu(0-output))) + soft*torch.sum(torch.square(torch.relu(output-100)))\n",
        "            loss += loss_val.item()            # accumulate\n",
        "            loss_val.backward()                # gradients\n",
        "            optimizer.step()                   # update paramters\n",
        "            count += 1\n",
        "\n",
        "        net.eval()\n",
        "        R2_qrh = np.append(R2_qrh, eval(net, test_ds, p)[0].item())\n",
        "        R2_msa = np.append(R2_msa, eval(net, test_ds, p)[1].item())\n",
        "        \n",
        "        if(early_stop(list = R2_qrh, min_epochs = min_epochs, patience = patience) == 1):\n",
        "            break\n",
        "    \n",
        "    return R2_qrh, R2_msa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     n_train  smooth  size      lr  best_epoch  R2_qrh  R2_msa\n",
            "0       32.0   0.000  16.0  0.0001         2.0 -1.3684 -1.3678\n",
            "1       32.0   0.000  16.0  0.0010         2.0 -1.3763 -1.3757\n",
            "2       32.0   0.000  16.0  0.0100         2.0 -1.3140 -1.3134\n",
            "3       32.0   0.000  16.0  0.1000         2.0 -1.1368 -1.1363\n",
            "4       32.0   0.000  32.0  0.0001         2.0 -1.3893 -1.3887\n",
            "5       32.0   0.000  32.0  0.0010         2.0 -1.4108 -1.4102\n",
            "6       32.0   0.000  32.0  0.0100         2.0 -1.3779 -1.3773\n",
            "7       32.0   0.000  32.0  0.1000         2.0 -0.8474 -0.8471\n",
            "8       32.0   0.000  64.0  0.0001         2.0 -1.4237 -1.4231\n",
            "9       32.0   0.000  64.0  0.0010         2.0 -1.4007 -1.4001\n",
            "10      32.0   0.000  64.0  0.0100         2.0 -1.3220 -1.3214\n",
            "11      32.0   0.000  64.0  0.1000         2.0 -0.5476 -0.5474\n",
            "12      32.0   0.000  16.0  0.0001         2.0 -1.4275 -1.4269\n",
            "13      32.0   0.000  16.0  0.0010         2.0 -1.3122 -1.3117\n",
            "14      32.0   0.000  16.0  0.0100         2.0 -1.3631 -1.3625\n",
            "15      32.0   0.000  16.0  0.1000         2.0 -1.0466 -1.0461\n",
            "16      32.0   0.000  32.0  0.0001         2.0 -1.4060 -1.4054\n",
            "17      32.0   0.000  32.0  0.0010         2.0 -1.3371 -1.3365\n",
            "18      32.0   0.000  32.0  0.0100         2.0 -1.3429 -1.3423\n",
            "19      32.0   0.000  32.0  0.1000         2.0 -1.0151 -1.0146\n",
            "20      32.0   0.000  64.0  0.0001         2.0 -1.3750 -1.3744\n",
            "21      32.0   0.000  64.0  0.0010         2.0 -1.3242 -1.3237\n",
            "22      32.0   0.000  64.0  0.0100         2.0 -1.2385 -1.2380\n",
            "23      32.0   0.000  64.0  0.1000         2.0 -0.7321 -0.7317\n",
            "24      32.0   0.001  16.0  0.0001         2.0 -1.3656 -1.3650\n",
            "25      32.0   0.001  16.0  0.0010         2.0 -1.3580 -1.3574\n",
            "26      32.0   0.001  16.0  0.0100         2.0 -1.3033 -1.3028\n",
            "27      32.0   0.001  16.0  0.1000         2.0 -1.1016 -1.1011\n",
            "28      32.0   0.001  32.0  0.0001         2.0 -1.4025 -1.4019\n",
            "29      32.0   0.001  32.0  0.0010         2.0 -1.3514 -1.3508\n",
            "30      32.0   0.001  32.0  0.0100         2.0 -1.2666 -1.2661\n",
            "31      32.0   0.001  32.0  0.1000         2.0 -0.9057 -0.9053\n",
            "32      32.0   0.001  64.0  0.0001         2.0 -1.3659 -1.3653\n",
            "33      32.0   0.001  64.0  0.0010         2.0 -1.3462 -1.3457\n",
            "34      32.0   0.001  64.0  0.0100         2.0 -1.2866 -1.2860\n",
            "35      32.0   0.001  64.0  0.1000         2.0 -0.5836 -0.5833\n",
            "36      32.0   0.001  16.0  0.0001         2.0 -1.3805 -1.3799\n",
            "37      32.0   0.001  16.0  0.0010         2.0 -1.3650 -1.3644\n",
            "38      32.0   0.001  16.0  0.0100         2.0 -1.3700 -1.3694\n",
            "39      32.0   0.001  16.0  0.1000         2.0 -1.2037 -1.2032\n",
            "40      32.0   0.001  32.0  0.0001         2.0 -1.3445 -1.3439\n",
            "41      32.0   0.001  32.0  0.0010         2.0 -1.3464 -1.3458\n",
            "42      32.0   0.001  32.0  0.0100         2.0 -1.3543 -1.3537\n",
            "43      32.0   0.001  32.0  0.1000         2.0 -1.1471 -1.1466\n",
            "44      32.0   0.001  64.0  0.0001         2.0 -1.3481 -1.3476\n",
            "45      32.0   0.001  64.0  0.0010         2.0 -1.3931 -1.3925\n",
            "46      32.0   0.001  64.0  0.0100         2.0 -1.2907 -1.2901\n",
            "47      32.0   0.001  64.0  0.1000         2.0 -0.5148 -0.5145\n",
            "48      32.0   0.010  16.0  0.0001         2.0 -1.3979 -1.3972\n",
            "49      32.0   0.010  16.0  0.0010         2.0 -1.4062 -1.4056\n",
            "50      32.0   0.010  16.0  0.0100         2.0 -1.3855 -1.3849\n",
            "51      32.0   0.010  16.0  0.1000         2.0 -1.0971 -1.0967\n",
            "52      32.0   0.010  32.0  0.0001         2.0 -1.3983 -1.3976\n",
            "53      32.0   0.010  32.0  0.0010         2.0 -1.3155 -1.3149\n",
            "54      32.0   0.010  32.0  0.0100         2.0 -1.3677 -1.3671\n",
            "55      32.0   0.010  32.0  0.1000         2.0 -0.8821 -0.8817\n",
            "56      32.0   0.010  64.0  0.0001         2.0 -1.3862 -1.3856\n",
            "57      32.0   0.010  64.0  0.0010         2.0 -1.3462 -1.3456\n",
            "58      32.0   0.010  64.0  0.0100         2.0 -1.3108 -1.3102\n",
            "59      32.0   0.010  64.0  0.1000         2.0 -0.5348 -0.5346\n",
            "60      32.0   0.010  16.0  0.0001         2.0 -1.3321 -1.3315\n",
            "61      32.0   0.010  16.0  0.0010         2.0 -1.4001 -1.3994\n",
            "62      32.0   0.010  16.0  0.0100         2.0 -1.3422 -1.3417\n",
            "63      32.0   0.010  16.0  0.1000         2.0 -1.0694 -1.0690\n",
            "64      32.0   0.010  32.0  0.0001         2.0 -1.3740 -1.3734\n",
            "65      32.0   0.010  32.0  0.0010         2.0 -1.3688 -1.3682\n",
            "66      32.0   0.010  32.0  0.0100         2.0 -1.3729 -1.3723\n",
            "67      32.0   0.010  32.0  0.1000         2.0 -1.0659 -1.0654\n",
            "68      32.0   0.010  64.0  0.0001         2.0 -1.3426 -1.3420\n",
            "69      32.0   0.010  64.0  0.0010         2.0 -1.3348 -1.3342\n",
            "70      32.0   0.010  64.0  0.0100         2.0 -1.3309 -1.3303\n",
            "71      32.0   0.010  64.0  0.1000         2.0 -0.8822 -0.8818\n",
            "72      64.0   0.000  16.0  0.0001         2.0 -1.3000 -1.2994\n",
            "73      64.0   0.000  16.0  0.0010         2.0 -1.3627 -1.3621\n",
            "74      64.0   0.000  16.0  0.0100         2.0 -1.3276 -1.3270\n",
            "75      64.0   0.000  16.0  0.1000         2.0 -1.0047 -1.0043\n",
            "76      64.0   0.000  32.0  0.0001         2.0 -1.3510 -1.3505\n",
            "77      64.0   0.000  32.0  0.0010         2.0 -1.3221 -1.3216\n",
            "78      64.0   0.000  32.0  0.0100         2.0 -1.3338 -1.3332\n",
            "79      64.0   0.000  32.0  0.1000         2.0 -0.8998 -0.8994\n",
            "80      64.0   0.000  64.0  0.0001         2.0 -1.3308 -1.3302\n",
            "81      64.0   0.000  64.0  0.0010         2.0 -1.3429 -1.3424\n",
            "82      64.0   0.000  64.0  0.0100         2.0 -1.2250 -1.2245\n",
            "83      64.0   0.000  64.0  0.1000         2.0 -0.4795 -0.4793\n",
            "84      64.0   0.000  16.0  0.0001         2.0 -1.3599 -1.3593\n",
            "85      64.0   0.000  16.0  0.0010         2.0 -1.3002 -1.2996\n",
            "86      64.0   0.000  16.0  0.0100         2.0 -1.3554 -1.3548\n",
            "87      64.0   0.000  16.0  0.1000         2.0 -1.2280 -1.2274\n",
            "88      64.0   0.000  32.0  0.0001         2.0 -1.3181 -1.3175\n",
            "89      64.0   0.000  32.0  0.0010         2.0 -1.3741 -1.3735\n",
            "90      64.0   0.000  32.0  0.0100         2.0 -1.2121 -1.2116\n",
            "91      64.0   0.000  32.0  0.1000         2.0 -1.0421 -1.0416\n",
            "92      64.0   0.000  64.0  0.0001         2.0 -1.2683 -1.2677\n",
            "93      64.0   0.000  64.0  0.0010         2.0 -1.2924 -1.2918\n",
            "94      64.0   0.000  64.0  0.0100         2.0 -1.2679 -1.2674\n",
            "95      64.0   0.000  64.0  0.1000         2.0 -0.4722 -0.4720\n",
            "96      64.0   0.001  16.0  0.0001         2.0 -1.3132 -1.3126\n",
            "97      64.0   0.001  16.0  0.0010         2.0 -1.3909 -1.3903\n",
            "98      64.0   0.001  16.0  0.0100         2.0 -1.3246 -1.3240\n",
            "99      64.0   0.001  16.0  0.1000         2.0 -1.1320 -1.1315\n",
            "100     64.0   0.001  32.0  0.0001         2.0 -1.3649 -1.3643\n",
            "101     64.0   0.001  32.0  0.0010         2.0 -1.3180 -1.3174\n",
            "102     64.0   0.001  32.0  0.0100         2.0 -1.2439 -1.2433\n",
            "103     64.0   0.001  32.0  0.1000         2.0 -0.8379 -0.8375\n",
            "104     64.0   0.001  64.0  0.0001         2.0 -1.4058 -1.4052\n",
            "105     64.0   0.001  64.0  0.0010         2.0 -1.2900 -1.2894\n",
            "106     64.0   0.001  64.0  0.0100         2.0 -1.2525 -1.2520\n",
            "107     64.0   0.001  64.0  0.1000         2.0 -0.4531 -0.4529\n",
            "108     64.0   0.001  16.0  0.0001         2.0 -1.4046 -1.4040\n",
            "109     64.0   0.001  16.0  0.0010         2.0 -1.3511 -1.3506\n",
            "110     64.0   0.001  16.0  0.0100         2.0 -1.3167 -1.3161\n",
            "111     64.0   0.001  16.0  0.1000         2.0 -1.0944 -1.0940\n",
            "112     64.0   0.001  32.0  0.0001         2.0 -1.3453 -1.3447\n",
            "113     64.0   0.001  32.0  0.0010         2.0 -1.3094 -1.3088\n",
            "114     64.0   0.001  32.0  0.0100         2.0 -1.3154 -1.3149\n",
            "115     64.0   0.001  32.0  0.1000         2.0 -0.8023 -0.8019\n",
            "116     64.0   0.001  64.0  0.0001         2.0 -1.3853 -1.3847\n",
            "117     64.0   0.001  64.0  0.0010         2.0 -1.3626 -1.3620\n",
            "118     64.0   0.001  64.0  0.0100         2.0 -1.2568 -1.2562\n",
            "119     64.0   0.001  64.0  0.1000         2.0 -0.4652 -0.4650\n",
            "120     64.0   0.010  16.0  0.0001         2.0 -1.3273 -1.3267\n",
            "121     64.0   0.010  16.0  0.0010         2.0 -1.3489 -1.3483\n",
            "122     64.0   0.010  16.0  0.0100         2.0 -1.2751 -1.2745\n",
            "123     64.0   0.010  16.0  0.1000         2.0 -1.0888 -1.0884\n",
            "124     64.0   0.010  32.0  0.0001         2.0 -1.3314 -1.3308\n",
            "125     64.0   0.010  32.0  0.0010         2.0 -1.3588 -1.3582\n",
            "126     64.0   0.010  32.0  0.0100         2.0 -1.2592 -1.2586\n",
            "127     64.0   0.010  32.0  0.1000         2.0 -0.8474 -0.8471\n",
            "128     64.0   0.010  64.0  0.0001         2.0 -1.3502 -1.3497\n",
            "129     64.0   0.010  64.0  0.0010         2.0 -1.3741 -1.3735\n",
            "130     64.0   0.010  64.0  0.0100         2.0 -1.2333 -1.2328\n",
            "131     64.0   0.010  64.0  0.1000         2.0 -0.4894 -0.4891\n",
            "132     64.0   0.010  16.0  0.0001         2.0 -1.3374 -1.3368\n",
            "133     64.0   0.010  16.0  0.0010         2.0 -1.3466 -1.3460\n",
            "134     64.0   0.010  16.0  0.0100         2.0 -1.2696 -1.2691\n",
            "135     64.0   0.010  16.0  0.1000         2.0 -1.2397 -1.2392\n",
            "136     64.0   0.010  32.0  0.0001         2.0 -1.3504 -1.3498\n",
            "137     64.0   0.010  32.0  0.0010         2.0 -1.3143 -1.3137\n",
            "138     64.0   0.010  32.0  0.0100         2.0 -1.2574 -1.2568\n",
            "139     64.0   0.010  32.0  0.1000         2.0 -0.9835 -0.9831\n",
            "140     64.0   0.010  64.0  0.0001         2.0 -1.3851 -1.3845\n",
            "141     64.0   0.010  64.0  0.0010         2.0 -1.3442 -1.3436\n",
            "142     64.0   0.010  64.0  0.0100         2.0 -1.2607 -1.2601\n",
            "143     64.0   0.010  64.0  0.1000         2.0 -0.7331 -0.7328\n",
            "144    128.0   0.000  16.0  0.0001         2.0 -1.3048 -1.3043\n",
            "145    128.0   0.000  16.0  0.0010         2.0 -1.2466 -1.2461\n",
            "146    128.0   0.000  16.0  0.0100         2.0 -1.2395 -1.2390\n",
            "147    128.0   0.000  16.0  0.1000         2.0 -1.0117 -1.0113\n",
            "148    128.0   0.000  32.0  0.0001         2.0 -1.2619 -1.2614\n",
            "149    128.0   0.000  32.0  0.0010         2.0 -1.2897 -1.2891\n",
            "150    128.0   0.000  32.0  0.0100         2.0 -1.2177 -1.2172\n",
            "151    128.0   0.000  32.0  0.1000         2.0 -0.8932 -0.8928\n",
            "152    128.0   0.000  64.0  0.0001         2.0 -1.2375 -1.2370\n",
            "153    128.0   0.000  64.0  0.0010         2.0 -1.2392 -1.2387\n",
            "154    128.0   0.000  64.0  0.0100         2.0 -1.1474 -1.1470\n",
            "155    128.0   0.000  64.0  0.1000         2.0 -0.3553 -0.3551\n",
            "156    128.0   0.000  16.0  0.0001         2.0 -1.3047 -1.3041\n",
            "157    128.0   0.000  16.0  0.0010         2.0 -1.3327 -1.3321\n",
            "158    128.0   0.000  16.0  0.0100         2.0 -1.1984 -1.1980\n",
            "159    128.0   0.000  16.0  0.1000         2.0 -1.0939 -1.0934\n",
            "160    128.0   0.000  32.0  0.0001         2.0 -1.2561 -1.2556\n",
            "161    128.0   0.000  32.0  0.0010         2.0 -1.2649 -1.2644\n",
            "162    128.0   0.000  32.0  0.0100         2.0 -1.2085 -1.2080\n",
            "163    128.0   0.000  32.0  0.1000         2.0 -0.9530 -0.9526\n",
            "164    128.0   0.000  64.0  0.0001         2.0 -1.3089 -1.3084\n",
            "165    128.0   0.000  64.0  0.0010         2.0 -1.2652 -1.2646\n",
            "166    128.0   0.000  64.0  0.0100         2.0 -1.1331 -1.1326\n",
            "167    128.0   0.000  64.0  0.1000         2.0 -0.7207 -0.7204\n",
            "168    128.0   0.001  16.0  0.0001         2.0 -1.2492 -1.2487\n",
            "169    128.0   0.001  16.0  0.0010         2.0 -1.2352 -1.2347\n",
            "170    128.0   0.001  16.0  0.0100         2.0 -1.2532 -1.2527\n",
            "171    128.0   0.001  16.0  0.1000         2.0 -1.0144 -1.0140\n",
            "172    128.0   0.001  32.0  0.0001         2.0 -1.2774 -1.2769\n",
            "173    128.0   0.001  32.0  0.0010         2.0 -1.2466 -1.2460\n",
            "174    128.0   0.001  32.0  0.0100         2.0 -1.2167 -1.2162\n",
            "175    128.0   0.001  32.0  0.1000         2.0 -0.7467 -0.7463\n",
            "176    128.0   0.001  64.0  0.0001         2.0 -1.3193 -1.3188\n",
            "177    128.0   0.001  64.0  0.0010         2.0 -1.2273 -1.2268\n",
            "178    128.0   0.001  64.0  0.0100         2.0 -1.1242 -1.1237\n",
            "179    128.0   0.001  64.0  0.1000         2.0 -0.4277 -0.4275\n",
            "180    128.0   0.001  16.0  0.0001         2.0 -1.1957 -1.1952\n",
            "181    128.0   0.001  16.0  0.0010         2.0 -1.3185 -1.3179\n",
            "182    128.0   0.001  16.0  0.0100         2.0 -1.2457 -1.2452\n",
            "183    128.0   0.001  16.0  0.1000         2.0 -0.9575 -0.9571\n",
            "184    128.0   0.001  32.0  0.0001         2.0 -1.2744 -1.2739\n",
            "185    128.0   0.001  32.0  0.0010         2.0 -1.2875 -1.2870\n",
            "186    128.0   0.001  32.0  0.0100         2.0 -1.2444 -1.2439\n",
            "187    128.0   0.001  32.0  0.1000         2.0 -1.0407 -1.0403\n",
            "188    128.0   0.001  64.0  0.0001         2.0 -1.2448 -1.2443\n",
            "189    128.0   0.001  64.0  0.0010         2.0 -1.2371 -1.2366\n",
            "190    128.0   0.001  64.0  0.0100         2.0 -1.2019 -1.2014\n",
            "191    128.0   0.001  64.0  0.1000         2.0 -0.7227 -0.7224\n",
            "192    128.0   0.010  16.0  0.0001         2.0 -1.2568 -1.2563\n",
            "193    128.0   0.010  16.0  0.0010         2.0 -1.2728 -1.2723\n",
            "194    128.0   0.010  16.0  0.0100         2.0 -1.2183 -1.2178\n",
            "195    128.0   0.010  16.0  0.1000         2.0 -0.9976 -0.9972\n",
            "196    128.0   0.010  32.0  0.0001         2.0 -1.2755 -1.2750\n",
            "197    128.0   0.010  32.0  0.0010         2.0 -1.2999 -1.2994\n",
            "198    128.0   0.010  32.0  0.0100         2.0 -1.2161 -1.2156\n",
            "199    128.0   0.010  32.0  0.1000         2.0 -0.8028 -0.8025\n",
            "200    128.0   0.010  64.0  0.0001         2.0 -1.2643 -1.2638\n",
            "201    128.0   0.010  64.0  0.0010         2.0 -1.2955 -1.2949\n",
            "202    128.0   0.010  64.0  0.0100         2.0 -1.1876 -1.1872\n",
            "203    128.0   0.010  64.0  0.1000         2.0 -0.4180 -0.4177\n",
            "204    128.0   0.010  16.0  0.0001         2.0 -1.2088 -1.2083\n",
            "205    128.0   0.010  16.0  0.0010         2.0 -1.2281 -1.2276\n",
            "206    128.0   0.010  16.0  0.0100         2.0 -1.2186 -1.2181\n",
            "207    128.0   0.010  16.0  0.1000         2.0 -1.0908 -1.0903\n",
            "208    128.0   0.010  32.0  0.0001         2.0 -1.2468 -1.2463\n",
            "209    128.0   0.010  32.0  0.0010         2.0 -1.2892 -1.2887\n",
            "210    128.0   0.010  32.0  0.0100         2.0 -1.1877 -1.1872\n",
            "211    128.0   0.010  32.0  0.1000         2.0 -0.9814 -0.9809\n",
            "212    128.0   0.010  64.0  0.0001         2.0 -1.2498 -1.2493\n",
            "213    128.0   0.010  64.0  0.0010         2.0 -1.2325 -1.2320\n",
            "214    128.0   0.010  64.0  0.0100         2.0 -1.2078 -1.2073\n",
            "215    128.0   0.010  64.0  0.1000         2.0 -0.3855 -0.3854\n"
          ]
        }
      ],
      "source": [
        "# main\n",
        "df_result = pd.DataFrame({'n_train':[], 'smooth':[], 'size':[], 'lr':[], 'best_epoch':[], 'R2_qrh':[], 'R2_msa':[]})\n",
        "\n",
        "for n_train in [32, 64, 128]:\n",
        "    for _smooth in [0, 0.001, 0.01]:\n",
        "        for _soft in [0, 1e3]:\n",
        "            for h in [16, 32, 64]:\n",
        "                for _lr in [0.0001, 0.001, 0.01, 0.1]:\n",
        "\n",
        "                    # Read data\n",
        "                    df = pd.read_csv('C:/Users/tln229/Downloads/Python/1. Building/data/HVAC_B90_102_exp_10m_20210424.csv')\n",
        "                    qrh = np.array(df['htg_valve_command']).reshape(-1,1)\n",
        "                    msa = np.array(df['airflow_desired']).reshape(-1,1)\n",
        "\n",
        "                    # LEAST SQUARE\n",
        "                    ones = np.ones(msa.shape)\n",
        "                    A = np.concatenate((qrh, ones), axis=1)\n",
        "                    b = np.copy(msa)\n",
        "                    p = np.linalg.lstsq(A, b, rcond=None)[0]\n",
        "\n",
        "                    # Create network\n",
        "                    device = torch.device(\"cpu\")\n",
        "                    net = Net(h).to(device)\n",
        "\n",
        "                    # Create Dataset and DataLoader objects\n",
        "                    src_file = 'C:/Users/tln229/Downloads/Python/1. Building/data/HVAC_B90_102_exp_10m_20210424.csv'\n",
        "                    train_ds = Data(src_file, start=0, end=n_train)\n",
        "                    test_ds  = Data(src_file, start=n_train, end=1600)\n",
        "\n",
        "                    # train\n",
        "                    R2_qrh, R2_msa = train(net, p, train_ds, test_ds, lr=_lr, min_epochs=500, max_epochs=100000, patience=300, smooth=_smooth, soft=_soft)\n",
        "\n",
        "                    # results\n",
        "                    # print('n train = %3d \\t smooth = %6.4f \\t layer size = %2d \\t lr = %6.4f \\t best_epoch = %5d \\t best_R2_qrh = %7.5f \\t best_R2_msa = %7.5f'\n",
        "                    #     % (n_train, _smooth, h, _lr, np.argmax(R2_qrh), np.max(R2_qrh), np.max(R2_msa)))\n",
        "                    df_result.loc[len(df_result)] = [n_train, _smooth, h, _lr, np.argmax(R2_qrh), np.max(R2_qrh), np.max(R2_msa)]\n",
        "\n",
        "with pd.option_context('display.max_rows', None,\n",
        "                       'display.max_columns', None,\n",
        "                       'display.precision', 4,\n",
        "                       ):\n",
        "    print(df_result)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "912d6611990680b3d240e982c9d50f3da4c776707cfd42695cf7d82c88d80956"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
