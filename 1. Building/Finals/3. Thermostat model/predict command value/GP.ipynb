{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch as gp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/tln229/Downloads/Python/1. Building/data/HVAC_B90_102_exp_10m_20210424.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGPModel(gp.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(MyGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module  = gp.means.ZeroMean()\n",
    "        self.covar_module = gp.kernels.ScaleKernel(gp.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x  = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gp.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    n_train     lr  R2_qrh  R2_msa\n",
      "0      32.0  0.001 -1.3518 -1.3512\n",
      "1      32.0  0.010 -1.3509 -1.3502\n",
      "2      32.0  0.020 -1.3498 -1.3491\n",
      "3      32.0  0.050 -1.3464 -1.3457\n",
      "4      64.0  0.001 -0.9702 -0.9695\n",
      "5      64.0  0.010 -0.9616 -0.9609\n",
      "6      64.0  0.020 -0.9518 -0.9511\n",
      "7      64.0  0.050 -0.9220 -0.9213\n",
      "8     128.0  0.001 -0.1791 -0.1784\n",
      "9     128.0  0.010 -0.1701 -0.1695\n",
      "10    128.0  0.020 -0.1603 -0.1597\n",
      "11    128.0  0.050 -0.1314 -0.1307\n"
     ]
    }
   ],
   "source": [
    "df_result = pd.DataFrame({'n_train':[], 'lr':[], 'R2_qrh':[], 'R2_msa':[]})\n",
    "for n_train in ([32, 64, 128]):\n",
    "    for lr in ([0.001, 0.01, 0.02, 0.05]):\n",
    "\n",
    "        # TRAIN SET\n",
    "        clg_sp       = np.array(df['clg_sp_current']).reshape(-1,1)[0: n_train]\n",
    "        htg_sp       = np.array(df['htg_sp_current']).reshape(-1,1)[0: n_train]\n",
    "        htg_clg_mode = 1*np.array(df['htg_clg_mode']).reshape(-1,1)[0: n_train]\n",
    "\n",
    "        sp_k   = htg_sp*htg_clg_mode + clg_sp*(1-htg_clg_mode)\n",
    "        Tz_k   = np.array(df['thermostat_room_temp']).reshape(-1,1)[0: n_train]\n",
    "        qrh_k  = np.array(df['htg_valve_command']).reshape(-1,1)[0: n_train]\n",
    "        qrh_k1 = np.array(df['htg_valve_command'])[0+1: n_train+1]\n",
    "\n",
    "        train_x  = np.concatenate((sp_k, Tz_k, qrh_k), axis=1)\n",
    "        train_x = torch.tensor(train_x, dtype=torch.float32)\n",
    "        train_y = torch.tensor(qrh_k1, dtype=torch.float32)\n",
    "\n",
    "        # TEST SET\n",
    "        clg_sp       = np.array(df['clg_sp_current']).reshape(-1,1)[n_train: 1600]\n",
    "        htg_sp       = np.array(df['htg_sp_current']).reshape(-1,1)[n_train: 1600]\n",
    "        htg_clg_mode = 1*np.array(df['htg_clg_mode']).reshape(-1,1)[n_train: 1600]\n",
    "\n",
    "        sp_k   = htg_sp*htg_clg_mode + clg_sp*(1-htg_clg_mode)\n",
    "        Tz_k   = np.array(df['thermostat_room_temp']).reshape(-1,1)[n_train: 1600]\n",
    "        qrh_k  = np.array(df['htg_valve_command']).reshape(-1,1)[n_train: 1600]\n",
    "        qrh_k1 = np.array(df['htg_valve_command'])[n_train+1: 1600+1]\n",
    "        msa_k1 = np.array(df['airflow_desired'])[n_train+1: 1600+1]\n",
    "\n",
    "        test_x  = np.concatenate((sp_k, Tz_k, qrh_k), axis=1)\n",
    "        test_x  = torch.tensor(test_x, dtype=torch.float32)\n",
    "        test_y  = torch.tensor(qrh_k1, dtype=torch.float32)\n",
    "        test_y2 = torch.tensor(msa_k1, dtype=torch.float32)\n",
    "\n",
    "        # initialize likelihood and model\n",
    "        likelihood = gp.likelihoods.GaussianLikelihood()\n",
    "        model      = MyGPModel(train_x, train_y, likelihood)\n",
    "\n",
    "        # Training\n",
    "        training_iter = 200000    # number of training iteration\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)       # optimizer\n",
    "        mll = gp.mlls.ExactMarginalLogLikelihood(likelihood, model)   # marginal likelihood\n",
    "\n",
    "        R2_qrh = np.array([], dtype=np.float32)\n",
    "        R2_msa = np.array([], dtype=np.float32)\n",
    "        for i in range(training_iter+1):\n",
    "            # TRAIN\n",
    "            model.train()                   # find the hyperparameters\n",
    "            likelihood.train()\n",
    "\n",
    "            optimizer.zero_grad()           # Zero gradients from previous iteration\n",
    "            output = model(train_x)         # Output from model\n",
    "            loss   = -mll(output, train_y)  # Calc loss and backprop gradients\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # EVAL\n",
    "            model.eval()\n",
    "            likelihood.eval()\n",
    "\n",
    "            with torch.no_grad(), gp.settings.fast_pred_var():\n",
    "                pred_y = likelihood(model(test_x))\n",
    "\n",
    "            # r2 qrh\n",
    "            r2 = r2_score(test_y.numpy(), pred_y.mean.numpy())\n",
    "            R2_qrh = np.append(R2_qrh, r2)\n",
    "            \n",
    "            # r2 msa\n",
    "            qrh = np.array(df['htg_valve_command']).reshape(-1,1)\n",
    "            msa = np.array(df['airflow_desired']).reshape(-1,1)\n",
    "\n",
    "                # LEAST SQUARE\n",
    "            ones = np.ones(msa.shape)\n",
    "            A = np.concatenate((qrh, ones), axis=1)\n",
    "            b = np.copy(msa)\n",
    "            p = np.linalg.lstsq(A, b, rcond=None)[0]\n",
    "            pred_msa = pred_y.mean.numpy()*p[0] + p[1]\n",
    "            r2_msa = r2_score(test_y2.numpy(), pred_msa)\n",
    "            R2_msa = np.append(R2_msa, r2_msa)\n",
    "        \n",
    "        # print(np.argmax(R2_test))\n",
    "        # print('n:%4d \\t lr:%5.3f \\t r2_qrh:%8.4f \\t r2_msa:%8.4f' % (n_train, lr, np.max(R2_qrh), np.max(R2_msa)))\n",
    "        df_result.loc[len(df_result)] = [n_train, lr, np.max(R2_qrh), np.max(R2_msa)]\n",
    "\n",
    "with pd.option_context('display.max_rows', None,\n",
    "                       'display.max_columns', None,\n",
    "                       'display.precision', 4,\n",
    "                       ):\n",
    "    print(df_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "912d6611990680b3d240e982c9d50f3da4c776707cfd42695cf7d82c88d80956"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
