{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4jzKgPZ5D81V"
      },
      "outputs": [],
      "source": [
        "# library\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "torch.manual_seed(1)\n",
        "np.random.seed(1)\n",
        "torch.set_printoptions(precision=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HAEV0sGeEEAE"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, s):\n",
        "        super(Net, self).__init__()\n",
        "        self.input   = torch.nn.Linear(3, s)\n",
        "        self.output  = torch.nn.Linear(s, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = torch.tanh(self.input(x))\n",
        "        z = self.output(z)\n",
        "        return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8x017P0OEFVH"
      },
      "outputs": [],
      "source": [
        "# Model error\n",
        "def eval(model, testset, p):\n",
        "    with torch.no_grad():\n",
        "        pred_Y = model(testset.x_data)\n",
        "    \n",
        "    # R^2 qrh\n",
        "    r2_qrh = r2_score(testset.y_data, pred_Y)\n",
        "\n",
        "    # R^2 msa\n",
        "    pred_msa = pred_Y*p[0] + p[1]\n",
        "    r2_msa = r2_score(testset.msa_data, pred_msa)\n",
        "    \n",
        "    return r2_qrh, r2_msa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GjhVDtdREKfV"
      },
      "outputs": [],
      "source": [
        "# Data\n",
        "class Data(torch.utils.data.Dataset):\n",
        "  def __init__(self, src_file, start=None, end=None):\n",
        "    df = pd.read_csv(src_file)\n",
        "    clg_sp       = np.array(df['clg_sp_current']).reshape(-1,1)[start: end]\n",
        "    htg_sp       = np.array(df['htg_sp_current']).reshape(-1,1)[start: end]\n",
        "    htg_clg_mode = 1*np.array(df['htg_clg_mode']).reshape(-1,1)[start: end]\n",
        "\n",
        "    sp_k   = htg_sp*htg_clg_mode + clg_sp*(1-htg_clg_mode)\n",
        "    Tz_k   = np.array(df['thermostat_room_temp']).reshape(-1,1)[start: end]\n",
        "    qrh_k  = np.array(df['htg_valve_position']).reshape(-1,1)[start: end]\n",
        "    qrh_k1 = np.array(df['htg_valve_position']).reshape(-1,1)[start+1: end+1]\n",
        "    msa_k1 = np.array(df['airflow_current']).reshape(-1,1)[start+1: end+1]\n",
        "    tmp_x  = np.concatenate((sp_k, Tz_k, qrh_k), axis=1)\n",
        "    \n",
        "    self.x_data   = torch.tensor(tmp_x,  dtype=torch.float32)\n",
        "    self.y_data   = torch.tensor(qrh_k1, dtype=torch.float32)\n",
        "    self.msa_data = torch.tensor(msa_k1, dtype=torch.float32)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.x_data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if torch.is_tensor(idx):\n",
        "      idx = idx.tolist()\n",
        "    inp  = self.x_data[idx]\n",
        "    outp = self.y_data[idx]\n",
        "    msa  = self.msa_data[idx]\n",
        "    sample = {'inp':inp, 'outp':outp, 'msa':msa}\n",
        "    return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Early stopping\n",
        "def early_stop(list, min_epochs, patience):\n",
        "    if(len(list) > min_epochs):\n",
        "        if(np.max(list[-patience:]) < 1.0001*np.max(list[0: -patience])):\n",
        "            return 1\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNRSA74gELy0",
        "outputId": "81e6dec5-e0ee-448f-c67b-22411ed38c4e"
      },
      "outputs": [],
      "source": [
        "# train function\n",
        "def train(net, p, train_ds, test_ds, lr=0.001, min_epochs=200, max_epochs=100000, patience=100, smooth=0, soft=0):\n",
        "    \n",
        "    loss_func  = torch.nn.MSELoss()\n",
        "    optimizer  = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "    train_ldr = torch.utils.data.DataLoader(train_ds, batch_size=256, shuffle=True)\n",
        "    R2_qrh = np.array([])\n",
        "    R2_msa = np.array([])\n",
        "    for _ in range(0, max_epochs+1):\n",
        "        net.train()\n",
        "        loss  = 0\n",
        "        count = 0\n",
        "        for (_, batch) in enumerate(train_ldr):\n",
        "            X    = batch['inp']\n",
        "            Y    = batch['outp']\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = net(X)                    # compute the output of the Network\n",
        "            loss_val = loss_func(output, Y) + smooth*loss_func(output, X[:,2].reshape(-1,1)) + soft*torch.sum(torch.square(torch.relu(0-output))) + soft*torch.sum(torch.square(torch.relu(output-100)))\n",
        "            loss += loss_val.item()            # accumulate\n",
        "            loss_val.backward()                # gradients\n",
        "            optimizer.step()                   # update paramters\n",
        "            count += 1\n",
        "\n",
        "        net.eval()\n",
        "        R2_qrh = np.append(R2_qrh, eval(net, test_ds, p)[0].item())\n",
        "        R2_msa = np.append(R2_msa, eval(net, test_ds, p)[1].item())\n",
        "        \n",
        "        if(early_stop(list = R2_qrh, min_epochs = min_epochs, patience = patience) == 1):\n",
        "            break\n",
        "    \n",
        "    return R2_qrh, R2_msa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     n_train  smooth  size      lr  best_epoch  R2_qrh  R2_msa\n",
            "0       32.0   0.000  16.0  0.0001         2.0 -1.3575 -1.3227\n",
            "1       32.0   0.000  16.0  0.0010         2.0 -1.3655 -1.3309\n",
            "2       32.0   0.000  16.0  0.0100         2.0 -1.3052 -1.2721\n",
            "3       32.0   0.000  16.0  0.1000         2.0 -1.1274 -1.0990\n",
            "4       32.0   0.000  32.0  0.0001         2.0 -1.3782 -1.3430\n",
            "5       32.0   0.000  32.0  0.0010         2.0 -1.3998 -1.3643\n",
            "6       32.0   0.000  32.0  0.0100         2.0 -1.3668 -1.3321\n",
            "7       32.0   0.000  32.0  0.1000         2.0 -0.8667 -0.8448\n",
            "8       32.0   0.000  64.0  0.0001         2.0 -1.4121 -1.3760\n",
            "9       32.0   0.000  64.0  0.0010         2.0 -1.3896 -1.3546\n",
            "10      32.0   0.000  64.0  0.0100         2.0 -1.3130 -1.2803\n",
            "11      32.0   0.000  64.0  0.1000         2.0 -0.5584 -0.5448\n",
            "12      32.0   0.000  16.0  0.0001         2.0 -1.4157 -1.3795\n",
            "13      32.0   0.000  16.0  0.0010         2.0 -1.3017 -1.2689\n",
            "14      32.0   0.000  16.0  0.0100         2.0 -1.3520 -1.3175\n",
            "15      32.0   0.000  16.0  0.1000         2.0 -1.0624 -1.0358\n",
            "16      32.0   0.000  32.0  0.0001         2.0 -1.3946 -1.3591\n",
            "17      32.0   0.000  32.0  0.0010         2.0 -1.3263 -1.2925\n",
            "18      32.0   0.000  32.0  0.0100         2.0 -1.3323 -1.2986\n",
            "19      32.0   0.000  32.0  0.1000         2.0 -1.0312 -1.0054\n",
            "20      32.0   0.000  64.0  0.0001         2.0 -1.3638 -1.3288\n",
            "21      32.0   0.000  64.0  0.0010         2.0 -1.3136 -1.2804\n",
            "22      32.0   0.000  64.0  0.0100         2.0 -1.2301 -1.1993\n",
            "23      32.0   0.000  64.0  0.1000         2.0 -0.7295 -0.7108\n",
            "24      32.0   0.001  16.0  0.0001         2.0 -1.3547 -1.3202\n",
            "25      32.0   0.001  16.0  0.0010         2.0 -1.3470 -1.3125\n",
            "26      32.0   0.001  16.0  0.0100         2.0 -1.2936 -1.2609\n",
            "27      32.0   0.001  16.0  0.1000         2.0 -1.0929 -1.0655\n",
            "28      32.0   0.001  32.0  0.0001         2.0 -1.3915 -1.3559\n",
            "29      32.0   0.001  32.0  0.0010         2.0 -1.3406 -1.3066\n",
            "30      32.0   0.001  32.0  0.0100         2.0 -1.2565 -1.2247\n",
            "31      32.0   0.001  32.0  0.1000         2.0 -0.9019 -0.8795\n",
            "32      32.0   0.001  64.0  0.0001         2.0 -1.3548 -1.3204\n",
            "33      32.0   0.001  64.0  0.0010         2.0 -1.3358 -1.3018\n",
            "34      32.0   0.001  64.0  0.0100         2.0 -1.2770 -1.2445\n",
            "35      32.0   0.001  64.0  0.1000         2.0 -0.5953 -0.5805\n",
            "36      32.0   0.001  16.0  0.0001         2.0 -1.3695 -1.3347\n",
            "37      32.0   0.001  16.0  0.0010         2.0 -1.3541 -1.3198\n",
            "38      32.0   0.001  16.0  0.0100         2.0 -1.3590 -1.3247\n",
            "39      32.0   0.001  16.0  0.1000         2.0 -1.1950 -1.1648\n",
            "40      32.0   0.001  32.0  0.0001         2.0 -1.3335 -1.2996\n",
            "41      32.0   0.001  32.0  0.0010         2.0 -1.3356 -1.3017\n",
            "42      32.0   0.001  32.0  0.0100         2.0 -1.3435 -1.3096\n",
            "43      32.0   0.001  32.0  0.1000         2.0 -1.1380 -1.1094\n",
            "44      32.0   0.001  64.0  0.0001         2.0 -1.3373 -1.3032\n",
            "45      32.0   0.001  64.0  0.0010         2.0 -1.3824 -1.3476\n",
            "46      32.0   0.001  64.0  0.0100         2.0 -1.2801 -1.2478\n",
            "47      32.0   0.001  64.0  0.1000         2.0 -0.5221 -0.5091\n",
            "48      32.0   0.010  16.0  0.0001         2.0 -1.3866 -1.3516\n",
            "49      32.0   0.010  16.0  0.0010         2.0 -1.3952 -1.3596\n",
            "50      32.0   0.010  16.0  0.0100         2.0 -1.3749 -1.3399\n",
            "51      32.0   0.010  16.0  0.1000         2.0 -1.0891 -1.0617\n",
            "52      32.0   0.010  32.0  0.0001         2.0 -1.3867 -1.3510\n",
            "53      32.0   0.010  32.0  0.0010         2.0 -1.3048 -1.2716\n",
            "54      32.0   0.010  32.0  0.0100         2.0 -1.3568 -1.3226\n",
            "55      32.0   0.010  32.0  0.1000         2.0 -0.8736 -0.8517\n",
            "56      32.0   0.010  64.0  0.0001         2.0 -1.3748 -1.3392\n",
            "57      32.0   0.010  64.0  0.0010         2.0 -1.3360 -1.3023\n",
            "58      32.0   0.010  64.0  0.0100         2.0 -1.3003 -1.2673\n",
            "59      32.0   0.010  64.0  0.1000         2.0 -0.5423 -0.5292\n",
            "60      32.0   0.010  16.0  0.0001         2.0 -1.3212 -1.2874\n",
            "61      32.0   0.010  16.0  0.0010         2.0 -1.3887 -1.3533\n",
            "62      32.0   0.010  16.0  0.0100         2.0 -1.3317 -1.2979\n",
            "63      32.0   0.010  16.0  0.1000         2.0 -1.0504 -1.0240\n",
            "64      32.0   0.010  32.0  0.0001         2.0 -1.3632 -1.3289\n",
            "65      32.0   0.010  32.0  0.0010         2.0 -1.3579 -1.3236\n",
            "66      32.0   0.010  32.0  0.0100         2.0 -1.3630 -1.3284\n",
            "67      32.0   0.010  32.0  0.1000         2.0 -1.0585 -1.0321\n",
            "68      32.0   0.010  64.0  0.0001         2.0 -1.3317 -1.2978\n",
            "69      32.0   0.010  64.0  0.0010         2.0 -1.3240 -1.2904\n",
            "70      32.0   0.010  64.0  0.0100         2.0 -1.3208 -1.2875\n",
            "71      32.0   0.010  64.0  0.1000         2.0 -0.8650 -0.8439\n",
            "72      64.0   0.000  16.0  0.0001         2.0 -1.2879 -1.2549\n",
            "73      64.0   0.000  16.0  0.0010         2.0 -1.3503 -1.3161\n",
            "74      64.0   0.000  16.0  0.0100         2.0 -1.3155 -1.2821\n",
            "75      64.0   0.000  16.0  0.1000         2.0 -0.9954 -0.9698\n",
            "76      64.0   0.000  32.0  0.0001         2.0 -1.3387 -1.3044\n",
            "77      64.0   0.000  32.0  0.0010         2.0 -1.3098 -1.2763\n",
            "78      64.0   0.000  32.0  0.0100         2.0 -1.3213 -1.2875\n",
            "79      64.0   0.000  32.0  0.1000         2.0 -0.8919 -0.8688\n",
            "80      64.0   0.000  64.0  0.0001         2.0 -1.3185 -1.2850\n",
            "81      64.0   0.000  64.0  0.0010         2.0 -1.3307 -1.2971\n",
            "82      64.0   0.000  64.0  0.0100         2.0 -1.2138 -1.1829\n",
            "83      64.0   0.000  64.0  0.1000         2.0 -0.4746 -0.4622\n",
            "84      64.0   0.000  16.0  0.0001         2.0 -1.3473 -1.3128\n",
            "85      64.0   0.000  16.0  0.0010         2.0 -1.2882 -1.2553\n",
            "86      64.0   0.000  16.0  0.0100         2.0 -1.3429 -1.3086\n",
            "87      64.0   0.000  16.0  0.1000         2.0 -1.2166 -1.1855\n",
            "88      64.0   0.000  32.0  0.0001         2.0 -1.3059 -1.2725\n",
            "89      64.0   0.000  32.0  0.0010         2.0 -1.3611 -1.3262\n",
            "90      64.0   0.000  32.0  0.0100         2.0 -1.2012 -1.1703\n",
            "91      64.0   0.000  32.0  0.1000         2.0 -1.0325 -1.0061\n",
            "92      64.0   0.000  64.0  0.0001         2.0 -1.2567 -1.2246\n",
            "93      64.0   0.000  64.0  0.0010         2.0 -1.2806 -1.2478\n",
            "94      64.0   0.000  64.0  0.0100         2.0 -1.2568 -1.2247\n",
            "95      64.0   0.000  64.0  0.1000         2.0 -0.4704 -0.4579\n",
            "96      64.0   0.001  16.0  0.0001         2.0 -1.3010 -1.2677\n",
            "97      64.0   0.001  16.0  0.0010         2.0 -1.3781 -1.3429\n",
            "98      64.0   0.001  16.0  0.0100         2.0 -1.3125 -1.2789\n",
            "99      64.0   0.001  16.0  0.1000         2.0 -1.1220 -1.0932\n",
            "100     64.0   0.001  32.0  0.0001         2.0 -1.3523 -1.3177\n",
            "101     64.0   0.001  32.0  0.0010         2.0 -1.3059 -1.2720\n",
            "102     64.0   0.001  32.0  0.0100         2.0 -1.2332 -1.2014\n",
            "103     64.0   0.001  32.0  0.1000         2.0 -0.8297 -0.8084\n",
            "104     64.0   0.001  64.0  0.0001         2.0 -1.3928 -1.3572\n",
            "105     64.0   0.001  64.0  0.0010         2.0 -1.2780 -1.2453\n",
            "106     64.0   0.001  64.0  0.0100         2.0 -1.2412 -1.2092\n",
            "107     64.0   0.001  64.0  0.1000         2.0 -0.4556 -0.4432\n",
            "108     64.0   0.001  16.0  0.0001         2.0 -1.3916 -1.3564\n",
            "109     64.0   0.001  16.0  0.0010         2.0 -1.3384 -1.3043\n",
            "110     64.0   0.001  16.0  0.0100         2.0 -1.3044 -1.2708\n",
            "111     64.0   0.001  16.0  0.1000         2.0 -1.0796 -1.0519\n",
            "112     64.0   0.001  32.0  0.0001         2.0 -1.3329 -1.2995\n",
            "113     64.0   0.001  32.0  0.0010         2.0 -1.2973 -1.2642\n",
            "114     64.0   0.001  32.0  0.0100         2.0 -1.3035 -1.2701\n",
            "115     64.0   0.001  32.0  0.1000         2.0 -0.7959 -0.7752\n",
            "116     64.0   0.001  64.0  0.0001         2.0 -1.3724 -1.3373\n",
            "117     64.0   0.001  64.0  0.0010         2.0 -1.3500 -1.3153\n",
            "118     64.0   0.001  64.0  0.0100         2.0 -1.2448 -1.2130\n",
            "119     64.0   0.001  64.0  0.1000         2.0 -0.4665 -0.4535\n",
            "120     64.0   0.010  16.0  0.0001         2.0 -1.3149 -1.2812\n",
            "121     64.0   0.010  16.0  0.0010         2.0 -1.3363 -1.3022\n",
            "122     64.0   0.010  16.0  0.0100         2.0 -1.2636 -1.2312\n",
            "123     64.0   0.010  16.0  0.1000         2.0 -1.0780 -1.0504\n",
            "124     64.0   0.010  32.0  0.0001         2.0 -1.3189 -1.2847\n",
            "125     64.0   0.010  32.0  0.0010         2.0 -1.3463 -1.3120\n",
            "126     64.0   0.010  32.0  0.0100         2.0 -1.2477 -1.2154\n",
            "127     64.0   0.010  32.0  0.1000         2.0 -0.8409 -0.8192\n",
            "128     64.0   0.010  64.0  0.0001         2.0 -1.3377 -1.3031\n",
            "129     64.0   0.010  64.0  0.0010         2.0 -1.3615 -1.3271\n",
            "130     64.0   0.010  64.0  0.0100         2.0 -1.2220 -1.1905\n",
            "131     64.0   0.010  64.0  0.1000         2.0 -0.4831 -0.4705\n",
            "132     64.0   0.010  16.0  0.0001         2.0 -1.3249 -1.2910\n",
            "133     64.0   0.010  16.0  0.0010         2.0 -1.3342 -1.3001\n",
            "134     64.0   0.010  16.0  0.0100         2.0 -1.2579 -1.2257\n",
            "135     64.0   0.010  16.0  0.1000         2.0 -1.2284 -1.1970\n",
            "136     64.0   0.010  32.0  0.0001         2.0 -1.3382 -1.3040\n",
            "137     64.0   0.010  32.0  0.0010         2.0 -1.3021 -1.2687\n",
            "138     64.0   0.010  32.0  0.0100         2.0 -1.2458 -1.2138\n",
            "139     64.0   0.010  32.0  0.1000         2.0 -0.9717 -0.9465\n",
            "140     64.0   0.010  64.0  0.0001         2.0 -1.3724 -1.3372\n",
            "141     64.0   0.010  64.0  0.0010         2.0 -1.3317 -1.2976\n",
            "142     64.0   0.010  64.0  0.0100         2.0 -1.2497 -1.2178\n",
            "143     64.0   0.010  64.0  0.1000         2.0 -0.7251 -0.7063\n",
            "144    128.0   0.000  16.0  0.0001         2.0 -1.2931 -1.2627\n",
            "145    128.0   0.000  16.0  0.0010         2.0 -1.2356 -1.2066\n",
            "146    128.0   0.000  16.0  0.0100         2.0 -1.2282 -1.1990\n",
            "147    128.0   0.000  16.0  0.1000         2.0 -1.0023 -0.9777\n",
            "148    128.0   0.000  32.0  0.0001         2.0 -1.2504 -1.2208\n",
            "149    128.0   0.000  32.0  0.0010         2.0 -1.2780 -1.2481\n",
            "150    128.0   0.000  32.0  0.0100         2.0 -1.2067 -1.1778\n",
            "151    128.0   0.000  32.0  0.1000         2.0 -0.8856 -0.8632\n",
            "152    128.0   0.000  64.0  0.0001         2.0 -1.2262 -1.1969\n",
            "153    128.0   0.000  64.0  0.0010         2.0 -1.2280 -1.1987\n",
            "154    128.0   0.000  64.0  0.0100         2.0 -1.1376 -1.1097\n",
            "155    128.0   0.000  64.0  0.1000         2.0 -0.3503 -0.3395\n",
            "156    128.0   0.000  16.0  0.0001         2.0 -1.2929 -1.2623\n",
            "157    128.0   0.000  16.0  0.0010         2.0 -1.3208 -1.2898\n",
            "158    128.0   0.000  16.0  0.0100         2.0 -1.1876 -1.1589\n",
            "159    128.0   0.000  16.0  0.1000         2.0 -1.0845 -1.0582\n",
            "160    128.0   0.000  32.0  0.0001         2.0 -1.2450 -1.2158\n",
            "161    128.0   0.000  32.0  0.0010         2.0 -1.2536 -1.2240\n",
            "162    128.0   0.000  32.0  0.0100         2.0 -1.1977 -1.1694\n",
            "163    128.0   0.000  32.0  0.1000         2.0 -0.9439 -0.9205\n",
            "164    128.0   0.000  64.0  0.0001         2.0 -1.2972 -1.2672\n",
            "165    128.0   0.000  64.0  0.0010         2.0 -1.2539 -1.2244\n",
            "166    128.0   0.000  64.0  0.0100         2.0 -1.1232 -1.0961\n",
            "167    128.0   0.000  64.0  0.1000         2.0 -0.7098 -0.6915\n",
            "168    128.0   0.001  16.0  0.0001         2.0 -1.2379 -1.2082\n",
            "169    128.0   0.001  16.0  0.0010         2.0 -1.2240 -1.1943\n",
            "170    128.0   0.001  16.0  0.0100         2.0 -1.2420 -1.2126\n",
            "171    128.0   0.001  16.0  0.1000         2.0 -1.0075 -0.9825\n",
            "172    128.0   0.001  32.0  0.0001         2.0 -1.2659 -1.2360\n",
            "173    128.0   0.001  32.0  0.0010         2.0 -1.2352 -1.2057\n",
            "174    128.0   0.001  32.0  0.0100         2.0 -1.2058 -1.1765\n",
            "175    128.0   0.001  32.0  0.1000         2.0 -0.7399 -0.7207\n",
            "176    128.0   0.001  64.0  0.0001         2.0 -1.3076 -1.2770\n",
            "177    128.0   0.001  64.0  0.0010         2.0 -1.2161 -1.1871\n",
            "178    128.0   0.001  64.0  0.0100         2.0 -1.1140 -1.0867\n",
            "179    128.0   0.001  64.0  0.1000         2.0 -0.4266 -0.4132\n",
            "180    128.0   0.001  16.0  0.0001         2.0 -1.1847 -1.1563\n",
            "181    128.0   0.001  16.0  0.0010         2.0 -1.3067 -1.2759\n",
            "182    128.0   0.001  16.0  0.0100         2.0 -1.2360 -1.2066\n",
            "183    128.0   0.001  16.0  0.1000         2.0 -0.9487 -0.9249\n",
            "184    128.0   0.001  32.0  0.0001         2.0 -1.2630 -1.2330\n",
            "185    128.0   0.001  32.0  0.0010         2.0 -1.2760 -1.2459\n",
            "186    128.0   0.001  32.0  0.0100         2.0 -1.2335 -1.2044\n",
            "187    128.0   0.001  32.0  0.1000         2.0 -1.0311 -1.0059\n",
            "188    128.0   0.001  64.0  0.0001         2.0 -1.2334 -1.2038\n",
            "189    128.0   0.001  64.0  0.0010         2.0 -1.2262 -1.1975\n",
            "190    128.0   0.001  64.0  0.0100         2.0 -1.1911 -1.1625\n",
            "191    128.0   0.001  64.0  0.1000         2.0 -0.7167 -0.6983\n",
            "192    128.0   0.010  16.0  0.0001         2.0 -1.2454 -1.2159\n",
            "193    128.0   0.010  16.0  0.0010         2.0 -1.2614 -1.2317\n",
            "194    128.0   0.010  16.0  0.0100         2.0 -1.2071 -1.1787\n",
            "195    128.0   0.010  16.0  0.1000         2.0 -0.9886 -0.9643\n",
            "196    128.0   0.010  32.0  0.0001         2.0 -1.2640 -1.2345\n",
            "197    128.0   0.010  32.0  0.0010         2.0 -1.2883 -1.2584\n",
            "198    128.0   0.010  32.0  0.0100         2.0 -1.2052 -1.1768\n",
            "199    128.0   0.010  32.0  0.1000         2.0 -0.7951 -0.7745\n",
            "200    128.0   0.010  64.0  0.0001         2.0 -1.2530 -1.2235\n",
            "201    128.0   0.010  64.0  0.0010         2.0 -1.2838 -1.2539\n",
            "202    128.0   0.010  64.0  0.0100         2.0 -1.1770 -1.1485\n",
            "203    128.0   0.010  64.0  0.1000         2.0 -0.4140 -0.4013\n",
            "204    128.0   0.010  16.0  0.0001         2.0 -1.1979 -1.1693\n",
            "205    128.0   0.010  16.0  0.0010         2.0 -1.2171 -1.1879\n",
            "206    128.0   0.010  16.0  0.0100         2.0 -1.2076 -1.1790\n",
            "207    128.0   0.010  16.0  0.1000         2.0 -1.0812 -1.0551\n",
            "208    128.0   0.010  32.0  0.0001         2.0 -1.2355 -1.2061\n",
            "209    128.0   0.010  32.0  0.0010         2.0 -1.2776 -1.2476\n",
            "210    128.0   0.010  32.0  0.0100         2.0 -1.1776 -1.1493\n",
            "211    128.0   0.010  32.0  0.1000         2.0 -0.9721 -0.9481\n",
            "212    128.0   0.010  64.0  0.0001         2.0 -1.2386 -1.2089\n",
            "213    128.0   0.010  64.0  0.0010         2.0 -1.2213 -1.1922\n",
            "214    128.0   0.010  64.0  0.0100         2.0 -1.1968 -1.1683\n",
            "215    128.0   0.010  64.0  0.1000         2.0 -0.3914 -0.3794\n"
          ]
        }
      ],
      "source": [
        "# main\n",
        "df_result = pd.DataFrame({'n_train':[], 'smooth':[], 'size':[], 'lr':[], 'best_epoch':[], 'R2_qrh':[], 'R2_msa':[]})\n",
        "\n",
        "for n_train in [32, 64, 128]:\n",
        "    for _smooth in [0, 0.001, 0.01]:\n",
        "        for _soft in [0, 1e3]:\n",
        "            for h in [16, 32, 64]:\n",
        "                for _lr in [0.0001, 0.001, 0.01, 0.1]:\n",
        "\n",
        "                    # Read data\n",
        "                    df = pd.read_csv('C:/Users/tln229/Downloads/Python/1. Building/data/HVAC_B90_102_exp_10m_20210424.csv')\n",
        "                    qrh = np.array(df['htg_valve_position']).reshape(-1,1)\n",
        "                    msa = np.array(df['airflow_current']).reshape(-1,1)\n",
        "\n",
        "                    # LEAST SQUARE\n",
        "                    ones = np.ones(msa.shape)\n",
        "                    A = np.concatenate((qrh, ones), axis=1)\n",
        "                    b = np.copy(msa)\n",
        "                    p = np.linalg.lstsq(A, b, rcond=None)[0]\n",
        "\n",
        "                    # Create network\n",
        "                    device = torch.device(\"cpu\")\n",
        "                    net = Net(h).to(device)\n",
        "\n",
        "                    # Create Dataset and DataLoader objects\n",
        "                    src_file = 'C:/Users/tln229/Downloads/Python/1. Building/data/HVAC_B90_102_exp_10m_20210424.csv'\n",
        "                    train_ds = Data(src_file, start=0, end=n_train)\n",
        "                    test_ds  = Data(src_file, start=n_train, end=1600)\n",
        "\n",
        "                    # train\n",
        "                    R2_qrh, R2_msa = train(net, p, train_ds, test_ds, lr=_lr, min_epochs=500, max_epochs=100000, patience=300, smooth=_smooth, soft=_soft)\n",
        "\n",
        "                    # results\n",
        "                    # print('n train = %3d \\t smooth = %6.4f \\t layer size = %2d \\t lr = %6.4f \\t best_epoch = %5d \\t best_R2_qrh = %7.5f \\t best_R2_msa = %7.5f'\n",
        "                    #     % (n_train, _smooth, h, _lr, np.argmax(R2_qrh), np.max(R2_qrh), np.max(R2_msa)))\n",
        "                    df_result.loc[len(df_result)] = [n_train, _smooth, h, _lr, np.argmax(R2_qrh), np.max(R2_qrh), np.max(R2_msa)]\n",
        "\n",
        "with pd.option_context('display.max_rows', None,\n",
        "                       'display.max_columns', None,\n",
        "                       'display.precision', 4,\n",
        "                       ):\n",
        "    print(df_result)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5 (tags/v3.10.5:f377153, Jun  6 2022, 16:14:13) [MSC v.1929 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "912d6611990680b3d240e982c9d50f3da4c776707cfd42695cf7d82c88d80956"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
